method: 0                       # 0 for SimCLR, 1 for MoCo

# moco specific
moco_dim: 128                   # feature dimension (default: 128)
moco_k: 65536                   # queue size; number of negative keys (default: 65536)
moco_m: 0.999                   # moco momentum of updating key encoder (default: 0.999)
moco_t: 0.07                    # softmax temperature (default: 0.07)
mlp: True                       # use mlp head
aug_plus: True                  # use moco v2 data augmentation
cos: True                       # use cosine lr schedule

# distributed training
nodes: 1
gpus: 1                         # I recommend always assigning 1 GPU to 1 node
dataparallel: 0                 # Use DataParallel instead of DistributedDataParallel
workers: 8
dataset_dir: "./datasets"       # path to dataset

# pretrain options
seed: 42                        # sacred handles automatic seeding when passed in the config
batch_size: 128
image_size: 224
lr: 0.03                        # initial learning rate
start_epoch: 0
epochs: 1                       # this need to be increased
dataset: 1                      # dataset type. 0 for IMAGENET, 1 for CIFAR10, and 2 for STL10. This needs to be expanded to include imagenet
first_pretrain: True 
second_pretrain: True 
momentum: 0.9                   # momentum of SGD solver
resume: ""                      # path to latest checkpoint (default: none)
global_step: 0

# arch options
resnet: "resnet50"
projection_dim: 64              # "[...] to project the representation to a 128-dimensional latent space"

# loss options
optimizer: "Adam"               # or LARS (experimental)
weight_decay: 1.0e-6            # "optimized using LARS [...] and weight decay of 10âˆ’6"
temperature: 0.5                # see appendix B.7.: Optimal temperature under different batch sizes

# reload options
model_path: "save"              # set to the directory containing `checkpoint_##.tar` 
epoch_num: 0                    # set to checkpoint number
reload: False                   # indicates whether to start the training from the checkpoint or not

# logistic regression options
logistic_batch_size: 256
logistic_epochs: 500

# finetuning options
finetune: False
finetune_batch_size: 128
finetune_lr: 0.03               # initial learning rate
finetune_start_epoch: 0
finetune_epochs: 10             # this need to be increased

# active learning options
al_batches: 10
al_batch_size: 64
al_lr: 0.0
do_al: True
al_backbone: "resnet18"

# target pretraining options
target_dataset: 2               # dataset type. 0 for IMAGENET, 1 for CIFAR10, 2 for UCMERCED,  3 for SKETCH, and  4 for CLIPART.
