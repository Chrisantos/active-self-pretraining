method: 0                       # 0 for SimCLR, 1 for MoCo

# moco specific
moco_dim: 128                   # feature dimension (default: 128)
moco_k: 65536                   # queue size; number of negative keys (default: 65536)
moco_m: 0.999                   # moco momentum of updating key encoder (default: 0.999)
moco_t: 0.07                    # softmax temperature (default: 0.07)
mlp: True                       # use mlp head
aug_plus: True                  # use moco v2 data augmentation
cos: True                       # use cosine lr schedule

# distributed training
nodes: 1
gpus: 1                         # I recommend always assigning 1 GPU to 1 node
workers: 8
dataset_dir: "./datasets"       # path to dataset

# pretrain options
seed: 42                        # sacred handles automatic seeding when passed in the config
batch_size: 32                 # the default should be 128. However, should be varied until better performance is achieved
image_size: 224
lr: 0.03                        # initial learning rate. This should be varied between 3.0e-3 to 3.0e-2 - If you use LARS, then the lr needs to be decayed
current_epoch: 0
start_epoch: 0
base_epochs: 5                  # this needs to be increased to about 800??
target_epochs: 1                # this needs to be increased to about 800??
dataset: 1                      # dataset type. 0 for IMAGENET, 1 for CIFAR10, and 2 for STL10. This needs to be expanded to include imagenet
base_pretrain: True 
target_pretrain: True 
momentum: 0.9                   # momentum of SGD solver
resume: ""                      # path to latest checkpoint (default: none)
global_step: 0

# arch options
resnet: "resnet50"
projection_dim: 64              # "[...] to project the representation to a 128-dimensional latent space"

# loss options
optimizer: "Adam"               # or LARS (experimental) If you use LARS, then the lr needs to be decayed
weight_decay: 1.0e-6            # "optimized using LARS [...] and weight decay of 1.0eâˆ’6"
temperature: 0.5                # see appendix B.7.: Optimal temperature under different batch sizes

# reload options
model_path: "save"              # set to the directory containing `checkpoint_##.tar` 
epoch_num: 0                    # set to checkpoint number
reload: False                   # indicates whether to start the training from the checkpoint or not

# logistic regression options
logistic_batch_size: 256
logistic_epochs: 500

# finetuning options
finetune: False
finetune_batch_size: 128        # this needs to be increased to 256
finetune_lr: 0.001              # initial learning rate. This should be varied between 1.0e-3 to 1.0e-2
finetune_start_epoch: 0
finetune_epochs: 10             # this needs to be increased to 90
finetune_momentum: 0.9          # momentum of SGD solver

# active learning options
al_epochs: 2                    # I would like to take 100 as the default, however for initial confirmation test, I would use 10
al_batches: 2                   # the default should be kept at 10
al_batch_size: 8                # I would like to keep the default at 32 or 64, but I made the current value 8 due to the cuda issue I am having. This seems to be the value that didn't produce an error
al_image_size: 224
al_lr: 0.1
al_weight_decay: 5.0e-4
do_al: True
al_backbone: "resnet18"         # change this to resnet18
al_finetune_data_ratio: 1       # this indicates the amount of the target data at each batch to be used for finetuning to get the topk
al_method: 0                    # 0 for least confidence, 1 for entropy, 2 for both
al_path_loss_file: "al_path_loss.pkl"

# target pretraining options
target_dataset: 3               # dataset type. 0 for IMAGENET, 1 for CIFAR10, 2 for UCMERCED,  3 for SKETCH, and  4 for CLIPART.
pretrain_path_loss_file: "pretrain_path_loss.pkl"
