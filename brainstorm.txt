- I need a way to tie an image to a label, but there is no label, so what do I do?
- Can I assign an id to each image?

- I could just create a list during eval and add each image (assign an id to it or use the image itself, which I think is better) with its loss.
- Sort the loss in descending order
- Create a little proxy model that would compute the top1k of the images sorted by their loss in decreasing order
- When you eventually narrow down these images, save 90% of them in the disk so you don't have to go through the process again.

- Things to check:
- Can I use an already loaded image to train a proxy model?
- Can I save these images in the disk and load it later when I want to do the second pretraining?

- The proxy model should use the weights from the first pretrain and it should operate on the target dataset. 

- Have a proxy model which would be trained using samples (top1k) from the eval that was made using the batch from the target pretrain. 
The weights of the trained proxy should be saved at each epoch and after all epochs, it should be used to evaluate the eval model that generates the top1k.
This process repeats itself until the data budget is reached. These samples used in training the proxy is what is saved and eventually used for the 
second pretraining



image_loss.pkl should contain 30,000 images
path_loss.pkl should contain 1000 images

ensure you go through the official simclr code to ensure it correlates with yours

TODO: 

- Check out the Decoupled Contrastive Learning Loss paper and code for the proposed approach to improve accuracy even with small batch size and epochs
- https://github.com/alibaba/EasyCV has a swav implementation that might work
- https://github.com/HobbitLong/SupContrast has a good implementation of simclr

ssh e_chrisantus@robcog.cs.okstate.edu
Eze@33963003

scp -r CLionProjects/Group-G cheze@csx2.cs.okstate.edu:.
rm -rf dir-name
sudo mv info.txt config/

To move folder content up one level
mv myfolder/* .

pwd

subl main.py # This opens main.py using sublime text

scp cc@ip:/home/cc/filename . # This copy's the file to the local current directory
cp filename /destination directory

ssh cc@129.114.108.45

ssh cc@192.5.87.154 

https://github.com/fastai/imagenette


Since the train_proxy trains the classifier iteratively by updating the pool after each iteration, you don't need this for the casl project
For the casl project use the finetune_trainer trained on long epochs, to generate the pretext task losses, then use the finetune to
get the new samples, once.

DCL:
epochs = 200 (for all datasets)
temperature = 0.1 (ImageNet-1k), 0.07 (Cifar10, Cifar100, STL)
optimizer = SGD with Cosine annealing scheduler
lr = 0.03 * batch size/256
backbone = resnet-18 (Cifar10, Cifar100, STL), resnet-50 (ImageNet-1k, ImageNet-100)
batch size = They experimented with 32 - 512, but stuck with 256 eventually for simclr, dcl, and dclw

SimCLR:
epochs = 500
temperature = 0.5
optimizer = Adam
lr = 1e-3
weight_decay=1e-6
backbone = resnet-50
batch size = 512
num_workers = 16
train drop last = True
test drop last = False
train pin memory = True
test pin memory = True
